\chapter{卷积神经网络的相关技术介绍}

\section{卷积神经网络的基础操作和训练}
卷积神经网络是一种前馈神经网络，利用反向传播算法进行训练，被广泛应用 于图像识别领域。卷积神经网络和普通的神经网络很像：都需要可以学习参数的神 经元；神经元都要接受输入并进行点积运算；神经元都含有非线性激活函数；都是 把数据最后转化成输出；都需要分类的损失函数等等。其不同点在于卷积神经网络 假设输入的是图像，使得我们可以充分利用这个特点来优化网络结构，从而更加有 效率的完成网络的前馈和减少整个网络的参数。 卷积神经网络是使用深度学习进行人脸特征提取的基础，接下来，笔者将从网 络结构的基本组成、常用激活函数、常用参数初始化方法、卷积神经网络的训练与 优化四个方面来介绍卷积神经网络的相关内容。



\subsection{卷积神经网络结构的基本组成}
卷积神经网络结构的基本组成单位是层，不同于普通神经网络的层类型的单一， 卷积神经网络具有非常丰富种类的层。每个卷积神经网络都或多或少包含数个、数 十个甚至上百上千个层。接下来笔者将按照功能介绍最常见的一些层。
\subsubsection{卷积层}
卷积层是卷积神经网络最核心的层，包含了卷积神经网络的绝大部分计算。每 一个卷积层都会使用多个不同参数的卷积核，每一个卷积核对输入图像局部感知，实 现参数共享。 卷积层相关的超参数包括：
1. N 卷积核的数目
2. Kh,Kw 高度方向和宽度方向上的卷积核大小
3. Sh,Sw 高度方向和宽度方向上的卷积步长
4. Ph,Pw 高度方向和宽度方向上的补零数

输入输出参数包括：
1. Cinput,Hinput,Winput 分别为输入通道数、输入高度、输入宽度
2. Coutput,Houtput,Woutput 分别为输出通道数、输出高度、输出宽度
其中输出参数由如下公式确定：
Woutput =
Winput−Kw+2Pw Sw
+1
Houtput =
Hinput−Kh+2Ph Sh
+1
Coutput = N
(2-1)

卷积层的参数包括权重W1,...,WN 和偏置项 B1,...,BN。第 i 个卷积核的Wi 包含 的参数个数 NWi 为 Kh×Kw×Cinput，Bi 包含的参数量 NBi 为 1，则卷积层的浮点运算 量可以用浮点乘法运算量表示为 N×NWi×Houtput×Woutput。 卷积层的计算公式如下： xlj = f ∑ i∈Mj xl−1 i ∗klij +blj  (2-2) 其中 Mj 代表的是第 l 层的第 j 个输出所对应的 l−1 层特征图上的所有映射的集合， xl−1 i 代表第 l−1 层的第 i 个映射图，klij 代表的是第 j 个卷积核的第 i 个通道的空间 图谱，blj 代表的是第 j 个卷积核的偏置项，xlj 代表的是第 l 层的第 j 个输出；运算符 号∗代表卷积运算，f (·) 代表的是激活函数。 最常使用的卷积层是卷积核大小为 3×3 的卷积层，3x3 卷积的计算过程示例如 图2-1所示。假设输入为 4×4 的特征图谱，卷积核大小为 3×3，假设补零为 0，步长
10

图 2-1 3×3 卷积的计算过程图 为 1，则输出特征图谱的计算方法如下： A = 1×a+0×b+0×c+0×d+1×e+1×f +0×g+1×h+1×i B = 0×a+0×b+1×c+1×d+1×e+0×f +1×g+1×h+0×i C = 0×a+1×b+1×c+0×d+1×e+1×f +1×g+0×h+0×i D = 1×a+1×b+0×c+1×d+1×e+0×f +0×g+0×h+1×i
(2-3)
除此之外，一些比较新颖的卷积类型也相继出现。比如经常用于降维和增加非线性 的 1×1 卷积[24]，全卷积网络中常使用的扩张卷积[25] 等。

\subsubsection{池化层}
池化（Pooling）层用于对输入进行降采样，使得卷积后的特征图谱的空间尺度 能够逐渐地减小，使得卷积神经网络在减少参数的同时防止了过拟合。 池化层的超参数包括：
1. Kh,Kw 高度方向和宽度方向上的池化核大小 2. Sh,Sw 高度方向和宽度方向上的池化步长
输入输出参数包括：
1. Cinput,Hinput,Winput 分别为输入通道数、输入高度、输入宽度 2. Coutput,Houtput,Woutput 分别为输出通道数、输出高度、输出宽度
11

其中输出参数由如下公式确定：
Woutput =
Winput−Kw Sw
+1
Houtput =
Hinput−Kh Sh
+1
Coutput =Cinput
(2-4)


池化层的计算公式如下：
xlj = down i∈Mj (xl−1 i ) (2-5) 其中 Mj 代表的是第 l 层的第 j 个输出所对应的 l−1 层特征图上的所有映射的集合， xl−1 i 代表第 l−1 层的第 i 个映射图，xlj 代表的是第 l 层的第 j 个输出；down(·) 代表 的是下采样函数。
图 2-2 最大值池化的计算过程图
下采样操作通常有三种类型，一种是最大型池化，一种是均值型池化，另一种 是随机型池化。其中最大型池化使用最为广泛，示例如图2-2。假设输入为 4×4 的特 征图谱，池化核大小为 2×2，步长为 2，则输出特征图谱的计算方法如下： A = max(2,0,0,1) B = max(1,0,3,0) C = max(0,2,1,4) D = max(5,0,0,1) (2-6)
12

\subsubsection{全连接层}
全连接全连接（Fully-connected，FC）层即输入和输出所有神经元之间都包含连 接，也就是普通神经网络最常用的层。 全连接层的输入大小假设为 Ninput, 输出大小假设为 Noutput，则全连接层包含的 参数量以及浮点运算量均可近似表示为 Ninput×Noutput。 全连接层的计算公式如下：
Xoutput =WTXinput +b (2-7)
其中 Xinput 代表全连接层的输入的二维矩阵表示，Xoutput 代表全连接层的输出的二维 矩阵表示，WT 代表权重参数的转置，b 代表偏置项参数。

\subsubsection{归一化层}
归一化层指的是对每一层的输出进行标准化操作，使得下一层的输入保持一个 较为稳定的分布。常用的标准化层有局部区域归一化（Local Region Normalization， LRN）层，批量标准化（BatchNomalization，BN）层[26] 等。

\subsubsection{决策层及损失函数}
决策层是整个网络的目标所在，包含着损失函数。
最常用的决策层损失函数 是 Softmax 损失函数和欧几里得损失函数。
 Softmax 损失函数主要用于多分类任务。假设决策层有 N 个类别目标，则 Softmax 分类器对应着 N 个输出节点，而这 N 个节点分别对应着 N 个离散的概率分布。 通过概率最大的类可以得到给出的预测类。相关公式如下所示：
pi =
ehi ∑N j=1ehj c = argmax i pi
(2-8)
其中 hi 代表第 i 个输出，pi 代表第 i 个概率值。从上述概率公式可以很快的得
13

到 Softmax 损失函数的表达式：
Jsoftmax =−
1 m
m ∑ i=1
N ∑ j=1[1{yi == j}log ehi ∑k=1Nehk]
1{yi == j}=  
1, yi == j 0, yi ! = j
(2-9)
其中 Jsoftmax 代表 Softmax 损失函数，m 为样本数量，N 为标签数量，yi 为第 i 个样本 的标签。 欧几里得损失函数主要用于回归任务。假设决策层的输出有 N 个值，则欧几里 得损失函数的公式为：
Jeuc =
1 2m
m ∑ i=1
N ∑ j=1
(hij−yij)2 (2-10)
其中其中 Jeuc 代表欧几里得损失函数,m 为样本数量,yij 为第 i 个样本的第 j 个标签， hij 为第 i 个样本的第 j 个输出。

\subsection{卷积神经网络常用激活函数}
激活函数是神经网络非线性的保证。与普通神经网络常使用的 Sigmoid、双曲正 切激活函数不一样，卷积神经网络中最常用的激活函数属于修正线性单元一族。下 面先介绍一下 Sigmoid、双曲正切激活函数，然后介绍卷积神经网络中最常用的几种 激活函数。
\subsubsection{Sigmoid 激活函数}
Sigmoid 激活函数是神经网络时代最常用的激活函数。Sigmoid 激活函数的数学 公式如下：
xoutput =
1 1+e−xinput
(2-11)
可以看出，Sigmoid 函数能把输出映射到区间 (0,1)：若输入非常大的负数，输出则 趋近于 0；若输入非常大的正数，则输出趋近于 1。近年来，Sigmoid 激活函数越来 越少被使用了，主要是由于Sigmoid激活函数很容易饱和：当输入很大或者很小的时 候，梯度是趋近于 0 的，如果初始值过大，会导致大量神经元处在饱和状态，即不更 新梯度，使得网络收敛越发困难。
14

\subsubsection{双曲正切激活函数}
双曲正切激活函数是神经网络时代另一个比较常用的激活函数。双曲函数激活 函数的数学公式如下：
xoutput =
exinput −e−xinput exinput +e−xinput
(2-12)

\subsubsection{修正线性单元激活函数}
修正式线性单元修正式线性单元（Rectified Linear Unit，ReLU）是 Hinton 等[4] 在 2012 年使用后开始广泛使用的，有着历史性的意义。修正式线性单元的实现很简 单，如下： xoutput =   xinput, xinput ≥ 0 0, xinput < 0 (2-13) 相较于 Sigmoid、双曲正切激活函数，由于自身的非饱和特性，修正式线性单元极大 程度的加速了深度卷积网络的收敛。但是修正式线性单元也存在一个很大的问题。 在训练的时候，修正式线性单元比较脆弱并且可能“死亡”。举个例子来说，当一个 很大的梯度流过修正式线性单元的神经元的时候，可能会导致梯度更新到一种特别 的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发 生，那么从此所以流过这个神经元的梯度将都变成 0。也就是说，这个单元在训练中 将不可逆转的死亡，而这样会导致数据多样化的丢失。
\subsubsection{带有负轴斜率的修正线性单元激活函数}
为了解决修正式线性单元的“死亡”问题，许多带有负轴斜率的修正式线性单 元方法相继出现，表达式为如下： xoutput =   xinput, xinput ≥ 0 α ·xinput, xinput < 0 (2-14) 其中：
1. 在漏式的修正式线性单元漏式的修正式线性单元（Leaky Rectified Linear Unit， LReLU）中， α 是一个固定的斜率；
15

2. 在随机修正线性单元随机的修正式线性单元（Random Rectified Linear Unit， RReLU） [27] 中， α 是一个随机的斜率，且满足 α ∈[lower,unpper]，其中 lower、 upper 为 α 的下界和上界。
3. 在带参数的修正线性单元带参数的修正式线性单元（ParametricRectifiedLinear Unit，PReLU） [28] 中， α 是一个可以学习的斜率。
带有负轴斜率的修正式线性单元一定程度上解决了修正式线性单元的“死亡”问题， 但是输出的分布还达不到足够理想的状态。

\subsubsection{指数线性单元激活函数}
指数线性单元指数式线性单元（Exponential Linear Unit，ELU） [29] 也是在负轴上 对修正线性单元进行优化，其表达式为： xoutput =   xinput, xinput ≥ 0 exinput −1, xinput < 0 (2-15) ELU 的输出具有很好的特性，比如其输出的均值近乎为 0，这使得网络更容易收敛。

\subsection{卷积神经网络常用的参数初始化方法}
神经网络求解的是局部最小值，一个好的参数初始化方法能使得卷积神经网络 收敛且收敛的更快。常用的卷积神经网络初始化方法有如下几种。
\subsection{常数初始化}
使用固定的常数初始化每个参数，常用来初始化的常数一般比较小，通常为 0。 常数初始化方法通常对偏置项所使用。
\subsubsection{均匀分布初始化}
假设参数服从在区间 [l,h] 上的均匀分布，进而为参数进行初始化。通常为权重 参数使用。Xavier 等在 2010 年提出的 Xavier 初始化方法[30] 就是一种均匀分布初始 化方法，其表达式为： W ∼U[−√ 6 ninput +noutput ,√ 6 ninput +noutput] (2-16)
16

其中W 为某一个卷积层的参数，ninput 为卷积层的输入的个数，noutput 为卷积层的输 出的个数，U[l,h] 代表服从区间 [l,h] 内的均匀分布。Xavier 初始化方法能够使得每 一层的输出方差尽量相等，从而让网络中的信息更好的流动。
\subsubsection{高斯分布初始化}
假设参数服从以 0 为均值， σ
为方差的高斯分布，为参数进行初始化。通常为 权重参数使用。KaimingHe 等在 2015 年提出的 MSRA 初始化方法[28] 就是一种高斯 分布初始化方法，其表达式为： W ∼N[0,√ 4 ninput +noutput] (2-17) 其中W 为某一个卷积层的参数，ninput 为卷积层的输入的个数，noutput 为卷积层的输 出的个数，N[ µ , σ ] 代表服从均值为 µ 方差为 σ 的高斯分布。实验证明，对于较深的 卷积神经网络，MSRA 初始化方法比 Xavier 初始化方法更容易收敛。
\subsection{卷积神经网络的训练与优化}
 通常对于包含 N 个数据的数据集 D ，优化的损失函数可以写成：
JW,b =
1 N
N ∑ i=1
fW,b(Xi)+
λ r(W) (2-18)
其中 fW,b(Xi) 代表的是第 i 个数据的损失，r(W) 是正则化项， λ
是正则化项的系数。 由于 N 有可能很大，所以实际上每一次的优化都只是使用一小部分的数据，通常称 为一个批量。 卷积神经网络最常用的优化方法是带动量的批量梯度下降批量梯度下降法 （BatchGradientDescent，BGD）法。带动量的批量梯度下降法的更新公式如下： Vt+1 = µ Vt− α ∇L(Wt) Wt+1 =Wt +Vt+1 (2-19) 其中 t+1 是迭代的当前轮数，W 是需要更新的参数，∇L(Wt) 是目标损失函数对于 Wt 的偏导数，Vt 是上一次参数的更新量，Vt+1 是本次的参数更新量， µ 是动量值， α 是学习率。 卷积神经网络的训练主要使用基于梯度的反向传播（Backpropagation，BP）算法。 假设卷积神经网络一共有 N 层，记作 L1,...,LN，y 代表样本标签，z 代表每一层的输
17

出，a 代表每一层输出的激活值， δ
代表每一层传回的梯度值，W 为权重，b 为偏置
项，则BP 算法步骤如下：
1. 进行前馈运算，利用前向传导公式，得到 L2,...,LN 的激活值；
2. 对 LN 层，计算:
δ LN =−(y−aLN)f′(zLN) (2-20) 3. 对于第 i 层 Li，i = N−1,...,2，计算： δ Li =((WLi)T δ Li+1)f′(zLi) (2-21) 4. 依次计算每一层参数的偏导值： ∇WLiJW,b = δ Li+1(aLi)T∇ bLiJW,b = δ Li+1 (2-22)
5. 利用公式2-19更新参数值。

\section{多机多卡策略对于网络训练的提升}

\subsection{基于机器学习框架的多机多卡训练}

\subsubsection{caffe}

\subsubsection{mxnet}

\subsubsection{tensroflow}


\subsection{基于底层MPI协议的GPU通信协调的多机多卡训练训练}
使用多GPU进行网络的训练在多种深度学习的框架都有实现，但是基层的实现都是基于nvidia-NCCL（Nvidia Collective multi-GPU Communication Library）的多GPU通信库。
NCCL是Nvidia Collective multi-GPU Communication Library的简称，它是一个实现多GPU的collective communication通信（all-gather, reduce, broadcast）库，Nvidia做了很多优化，以在PCIe、Nvlink、InfiniBand上实现较高的通信速度。
下面分别从以下几个方面来介绍NCCL的特点，包括基本的communication primitive、ring-base collectives、NCCL在单机多卡上以及多机多卡实现、最后分享实际使用NCCL的一些经验。
\subsubsection{communication primitive}
并行任务的通信一般可以分为Point-to-point communication和Collective communication。P2P通信这种模式只有一个sender和一个receiver，实现起来比较简单。第二种Collective communication包含多个sender多个receiver，一般的通信原语包括broadcast，gather,all-gather,scatter,reduce,all-reduce,reduce-scatter,all-to-all等。简单介绍几个常用的操作：
Reduce：从多个sender那里接收数据，最终combine到一个节点上。
All-reduce：从多个sender那里接收数据，最终combine到每一个节点上。
而传统Collective communication假设通信节点组成的topology是一颗fat tree，如下图所示，这样通信效率最高。但实际的通信topology可能比较复杂，并不是一个fat tree。因此一般用ring-based Collective communication。
\subsubsection{ring-base collectives}
ring-base collectives将所有的通信节点通过首尾连接形成一个单向环，数据在环上依次传输。以broadcast为例， 假设有4个GPU，GPU0为sender将信息发送给剩下的GPU，按照环的方式依次传输，GPU0-->GPU1-->GPU2-->GPU3，若数据量为N，带宽为B，整个传输时间为（K-1）N/B。时间随着节点数线性增长，不是很高效。
下面把要传输的数据分成S份，每次只传N/S的数据量，传输过程如下所示：
GPU1接收到GPU0的一份数据后，也接着传到环的下个节点，这样以此类推，最后花的时间为
S*(N/S/B) + (k-2)*(N/S/B) = N(S+K-2)/(SB) --> N/B，条件是S远大于K，即数据的份数大于节点数，这个很容易满足。所以通信时间不随节点数的增加而增加，只和数据总量以及带宽有关。其它通信操作比如reduce、gather以此类推。
那么在以GPU为通信节点的场景下，怎么构建通信环呢？如下图所示：
单机4卡通过同一个PCIe switch挂载在一棵CPU的场景：
单机8卡通过两个CPU下不同的PCIe switch挂载的场景：
\subsubsection{NCCL实现}
NCCL实现成CUDA C++ kernels，包含3种primitive operations： Copy，Reduce，ReduceAndCopy。目前NCCL 1.0版本只支持单机多卡，卡之间通过PCIe、NVlink、GPU Direct P2P来通信。NCCL 2.0会支持多机多卡，多机间通过Sockets (Ethernet)或者InfiniBand with GPU Direct RDMA通信。
下图所示，单机内多卡通过PCIe以及CPU socket通信，多机通过InfiniBand通信。
同样，在多机多卡内部，也要构成一个通信环。
下面是单机 4卡（Maxwel GPU）上各个操作随着通信量增加的带宽速度变化，可以看到带宽上限能达到10GB/s，接近PCIe的带宽。
下图是Allreduce在单机不同架构下的速度比较：
先不看DGX-1架构，这是Nvidia推出的深度学习平台，带宽能达到60GB/s。前面三个是单机多卡典型的三种连接方式，第三种是四张卡都在一个PCIe switch上，所以带宽较高，能达到>10GB/s PCIe的带宽大小，第二种是两个GPU通过switch相连后再经过CPU连接，速度会稍微低一点，第一种是两个GPU通过CPU然后通过QPI和另一个CPU上的两块卡相连，因此速度最慢，但也能达到>5GB/s。
下图是Allreduce多机下的速度表现，左图两机8卡，机内PCIe，机间InfiniBand能达到>10GB/s的速度，InfiniBand基本上能达到机内的通信速度。
下图是NCCL在CNTK ResNet50上的scalability，32卡基本能达到线性加速比。



\section{网络前馈速度的优化}
网络前馈又被称为网络的inference，一般是指经过一定的数据训练，对于输入数据和输出结果具有一定正向的科学计算过程，可能这样的说法不是很准确，因为很多时候为了获得更为理想的判断结果，往往会采取人工检查和机器过滤的两种方式进行结合，那么在我的工作之中主要是对于机器过滤中对于所需要使用的科学计算过程所进行的一些速度上的优化。具体算法的表述形式可以参照2.1节中的神经网络的基础算法过程进行对比。
\subsection{卷积计算的优化方式}
对于卷积层的优化有三个方向，对于直接计算卷积的过程进行优化，使用第三方的加速库对卷积计算进行加速
\subsubsection{直接进行卷积的算法优化}
CPU 直接计算卷积

使用矢量化编程加速卷积的直接计算方式

\subsubsection{借助第三方的计算库对于卷积计算进行优化}
使用imcol+gemm的方式进行计算，借助openblas、MKL、altblas等

使用 MKLDNN中的卷积层实现

使用NNPACK中的FFT卷积操作

使用CUDNN和TensorRT中的卷积实现

\subsection{不同网络层的合并}
\subsubsection{scale层和batch norm层的合并}

\subsubsection{BN层和卷积层的合并}

\subsubsection{卷积层和relu层的合并}






