\chapter{对抗神经网络在人脸属性中的应用}
\section{对抗生成网络相关技术的介绍}
对抗生成网络自2014年由Goodfellow提出，当时由于其效果并不是很显著的原因一直导致学术界没有过多的关注，自从2015年末到2016年末，在深度学习的蓬勃发展下，对抗生成网络也借此春风卷土重来，而且相关媒体预测，在2017年，深度对抗生成网络将在这一年会出现更多的研究成果。
思想和训练方法

对抗生成网络(Generative Adversarial Networks,下称GAN)启发自博弈论中的二人零和博弈(two-player game),在二人零和博弈中，两位博弈方的利益之和为零或一个常数，即一方有所得，另一方必有所失。而在对抗生成网络中，这两个博弈方的角色会分别有生成式模型(generative model)和判别式模型(discriminative model)充当.具体方式为：生成模型G捕捉样本数据的分布，判别模型D时一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率。G和D为一般的非线性映射函数，例如多层感知机、卷积神经网络等，这样，就找到了一个可以和深度学习相互结合的点。 ### 数学思想 假设有一种概率分布M，它相对于我们是一个黑盒子。为了了解这个黑盒子中的东西是什么，这里构建了两个东西G和D，G是另一种我们完全知道的概率分布，D用来区分一个事件是由黑盒子中那个不知道的东西产生的还是由我们自己设的G产生的。

通过不断调整G和D，直到D不能把事件区分出来为止。在调整过程中，需要：

优化G，使得它尽可能的让D混淆
优化D，使得它尽可能的能区分出假冒的东西
当D无法区分出事件的来源的时候，可以认为，G和M是一样的。从而，我们就了解到了黑盒子中的东西。

CGAN5

这篇论文是由著名的斯坦福视觉实验室出产的一篇文章，也就是网上盛传的李飞飞实验室。这篇文章首次提出为GAN增加限制条件，从而增加GAN的准确率。之前提到过，Goodfellow之所以在提出GAN的时候没有产生轰动，其原因就是原始的GAN所产生的数据模糊不清，根本没有实用的价值，下图是原始GAN的效果图：
为了解决GAN太过自由这个问题，一个很自然的想法就是给GAN加一些约束，于是便有了这篇Conditional Generative Adversarial Nets,这篇工作的改进非常straightforward（简单粗暴），但被证明非常有效：
在生成模型和判别模型分别为数据加上标签，也就是加上了限制条件。 ### 生成效果

DCGAN6

DCGAN全称为Deep convolutional generative adversarial networks,即将深度学习中的卷积神经网络应用到了对抗神经网络中，这篇文章在工程领域内的意义及其大，解决了很多工程性的问题，再加上其源码的开放，将其推向了一个高峰。

文章共分4个部分，以下进行详细介绍 ### 提出并评估了卷积GANs在结构拓补方面约束条件，使其更加稳定。 针对CNN的GAN在结构上应该注意三部分的约束 - All convolutional net(Springenberg et al., 2014)

判别模型：使用带步长的卷积取代空间池化(spatial pooling),学习下采样 生成模型：使用微步幅卷积（fractional strided）,学习上采样

Eiminating fully connected layers
全局平均pooling有助于模型的稳定性，但损害收敛速度。 生成模型：输出层用Tanh函数，其它层用Relu激活函数 判别模型：所有层使用LeakyRelu

Batch Normalization (Ioffe & Szegedy, 2015)
证明了生成模型初始化的重要性，避免生成模型崩溃，即生成的所有样本都在一个点上（样本的均值和方差一致）

具体如下图所示：

该算数的具体解释为一张女人有笑脸的图片-一张女人图+一张男人图 = 一张有笑脸的男人图。

总结

这个模型为工业界具体使用CNN的对抗生成网络提供了非常完善的解决方案，并且生成的图片效果质量精细，为之后GAN的后续再应用领域的发展奠定了很好的基础，当然也可以说提供了一个标杆。

iGAN是在GAN领域中应用方面我见过最好的一篇文章，完美的将DCGAN和manifold learning融合在一起，以下是一个效果展示： iGAN 视频链接

原理介绍

原理图 如上图所示，该网络通过将一张图片以manifold learning的方式投影转化为一个可操作的流形，然后通过用户进行修改编辑，本质上其实就是为GAN中的Generator增加一个或多个约束条件，然后通过利用DCGAN生成目标图片，但此时图片仍然是一个流形，作者通过使用dense correspondence algorithm将流形转化为原始图片。

这里涉及到三个步骤，将x通过manifold转化为z，将z通过生成模型转化为x，再通过判别模型鉴别x是否为真实图片（和训练数据一致的图片）。

文章这里分了三个部分，听我一一道来。

Projecting an Image onto the Manifold

在这一部分，你需要知道什么是manifold , 以及当下比较流行的manifold learning的常用方法。具体可以参见知乎文章

这一部分主要的目的是将一张原始的图片转化一个可以操作的manifold.

文章对比了三种方法,这里主要阐述作者最终所使用的方法。

Projection via a feedforward network:作者通过训练一个前向神经网络P(x;θP)P(x;θP)来直接预测从原始图片x到隐随机变量z，训练的主要公式为： feedforward network 这里xRnxnR表示数据集中第n个原始图像，模型P的结构等价于对抗生成网络中判别式模型D的结构，不同的地方只是在最后一层输出层上，如果你熟知auto-encoder的话，你就可以将P想象为一个encoder , 而D是一个Decoder。这个方法的意思就是通过模型P预测出真实图片对应的latent vector z,然后使用生成模型G来生成一张可以操作的图片，此时G(z)G(z)即为manifold图片，也就是可以有用户操作的图片。以上是网络P的方法。

而真正的方法是首先给定一个真实的图片xRxR，我们首先使用上述提及的网络R(xR;θP)R(xR;θP)对真实图片xRxR求出一个latent vector z , 然后通过下面的公式进行优化，求出最终的z以及最终的G(z). 公式

Manipulating the Latent Vector

在该部分，开始对manifold进行操作，这里的操作相当于添加约束条件fg(x)=vgfg(x)=vg,而这里将操作定义为gg,包括对颜色的改变、对形状的改变、以及笔刷限制。因此给定一个初始的投影x0x0,通过添加约束条件后找出新的xϵMxϵM。具体公式如下： 公式 其变式如下： 变式 其中ED=λDlog(1−D(G(z)))ED=λDlog(1−D(G(z)))

在每次对manifold修改后，对该式进行梯度下降优化，从而实现对manifold的更改。 效果

Edit Transfer

上面的操作得到的最终结果是修改的流形，那么如何将质量很低的流形直接转换为原始的图片呢，本部分就是要解决这个问题。 首先要知道，在两个生成的图片G(z0)G(z0)和G(z1)G(z1)，我们可以生成一个中间序列[G((1−tN)⋅+tN⋅z1)]Nt=0[G((1−tN)⋅+tN⋅z1)]t=0N

Motion + Color flow algorithm: We then estimate the color and geometric changes by generalizing the brightness constancy assumption in traditional optical flow methods. 公式 其中I(x,y,t)I(x,y,t)表示生成的图片[G((1−tN)⋅+tN⋅z1)]Nt=0[G((1−tN)⋅+tN⋅z1)]t=0N中每个像素(x,y)(x,y)的RGB值(r,g,b,1)T(r,g,b,1)T， (u,v)(u,v)表示每次的改变t所对应的latent vector。A是一个3x4的color affine transformation matrix.

我们通过估计相邻帧的改变，并将逐帧的改变相连，这样就可以获得任何两个帧范围内的所有改变。 示意图

Transfer edits to the original photo:通过获得一个图像序列的改变估计，我们想这些改变运用到原始的图片，并产生一个对应的修改后的图片。（其实最主要的还是要获取关于“改变操作的估计”）

总结

本文章很好的展现了一个DCGAN在实践应用方面的具体案例，同时也在生成图像方面做出了巨大的贡献，巧妙的将manifold learning相关内容应用到GAN中，将交互这种可能性实现，这对将来类似的应用提供了很好的模板

LAPGAN8

LAPGAN江湖人称拉普拉斯对抗生成网络，该文章主要致力于生成更加清晰，更加锐利的数据。

LAPGAN还有个别称，是LAPCGAN，事实上该文章受启发与CGAN，同样在训练生成模型的时候加入了conditional variable,这也是本案例成功的一大重要原因。

本文章主要的贡献是解决GAN本身过于自由不可控这个问题，在CGAN中，通过引入条件变量yy(conditional variable y),使用额外信息y对模型增加条件，可以指导数据生成过程。如果条件变量y是类别标签，，可以将CGAN是把纯无监督的GAN变成有监督的模型的一种改进。这个简单直接的改进被证明非常有效,并广泛用于后续的相关工作中。

为了改进GAN太过自由的问题，还有一个想法就是不要让GAN一次完成全部任务，而是一次生成图像的一部分，分多次生成一张完整的图片。

该文章出自facebook，在视线sequential version的方式上，LAPGAN这个工作采用的是一种类似拉普拉斯金字塔(Laplacian Pyramid)的方式，因此命名为LAPGAN。

这个方式主要的操作就是downsample 和 upsample，而优势是每次只考虑样本和生成图像之间的残差的学习效果，某种程度上和Residual Network的思路是一样的，针对残差的逼近和学习，相对更加容易。于是吗，便有了如下过程： LAPGAN训练流程 在上图中，当图像为较大尺寸的时候，便开始Laplacian Pyramid过程，并且每一个process step时（每个Pyramid level）,传给D的只是针对残差的compare.另一方面，当图像采样到足够小的时候，也就是上图最右的step，则不再需要进行upsample和downsample的过程，这时给D的传送则是未经过处理的样本和生成的图像了。值得注意的是，每一层的生成模型其实是conditional的，图中橙色的箭头，将经过downsampl后再upsample后的图像作为label传入到D中，而且在每一层的G中，其输入也会将经过两次采样的图像加入进去。另外，每一步的GAN都是Independently trained的。

下面两图是关于训练过程和生成过程的描述： Sampling procedure Training procedure 过程就不赘述了，图中的说明文字足可以看懂。

总结

本篇文章从另一个角度来限制GAN的自由，通过金字塔的方式，将一幅图片的每一部分（每次采样）进行学习，使得生成的图片更加精细锐利，如果之后会涉及到生成高分辨率的图像的时候，未尝不可以考虑使用本方法，之前看过使用纯卷积神经网络来生成高分辨率的图像，其中代表作就是github上著名的waifu2x,还有下图的SRCNN: SRCNN 虽然SRCNN提到过patch based , 但是毕竟没有像LAPGAN一样为每个path单独建一个网络模型来提高分辨率，嗯，仔细想一想这个网络貌似可以在super-resolution大有可为呢。

SimGAN4

说到GAN就不得不提一下2016年12月Apple在人工智能领域发出的第一篇文章了，从这篇文章就能看出当下的人工智能领域有多火，连苹果都赶紧跑出来分一杯羹，好了，废话不多说，开始看文章。

Apple这篇文章很本质的利用GAN可以产生通训练数据质量一样的生成数据这个特性，通过GAN生成大量的和训练数据一样真实的数据，从而解决当前大规模的精确标注数据难以获取，人工标注成本过高等一系列问题，嗯，Apple这种企业还是蛮偏向于业务的。

下图是这篇文章主要解决的一个问题： task 本篇文章主要提出一个Simulated+Unsupervised(S+U) learning,这个目的是为了使用没有标注的真实数据通过一个模拟器来提高合成图片的真实度。

原理

原理架构 原理如上图所示，文章以眼睛数据为例，大家都知道，对于计算机来说，合成一张图像的成本其实很低，可以几段代码就可以合成成千上万张图片，这就是上图中的simulator,模拟器，但是它的输出，也就是合成的数据，这个数据本身真实性不够，对于检测相关的算法来说，如果训练数据是这些玩意，那训练出来的算法对于现实世界来说并没有什么用，专业的说法是这个算法的泛化（generalization）能力不足，很多这样的算法都期望输入大量的有精确标注的真实数据，但是这就有个缺点，成本太高了，所以GAN的出现让作者眼前一亮，通过使用没有标注的真实数据（这些数据网上遍地都是），再加上合成的数据，二者通过GAN进行训练，训练处的生成模型，也就是文章中提到的Refiner网络，就可以生成很多很多真实数据了，这岂不是很nice？文章在后面还通过gaze estimation的实验证明了这个方式的可行性。下面详细介绍原理吧。

S+U Learning with SimGAN

首先进行一系列定义，无label的真实数据yiϵyyiϵy,该数据用来学习出一个refiner Rθ(x)Rθ(x)来精炼一个合成数据xx,这里的θθ表示函数的参数，而x^x^表示由R所提炼出的图像：
x^:=Rθ(x)
x^:=Rθ(x)
,而这个网络的关键在于保持合成数据的标注的情况下，让合成数据看起来更加真实。因此损失函数为： Loss func 其中xixi是第i个合成的训练图像，而x^ix^i是其对应的精炼过的图像，上式中第一部分的损失表示为合成图像添加真实性，而第二部分则是为了保持合成图像的标注，也就是尽量减少合成的图像和精炼过的图像之间的差异。

当然上面只是GAN中的生成模型部分，那判别模型呢？

DϕDϕ表示判别网络，该网络用来判别图像是真实的数据还是由精炼模型所产生的数据。其数学模型如下： 判别式模型 该式等价于二分类问题的相对熵错误，其中Dϕ(⋅)Dϕ(⋅)表示输入图像是合成数据的可能性，而1−Dϕ(⋅)1−Dϕ(⋅)表示来自真实数据的可能性，文字使用一个ConvNet来实现判别器，该网络的最后一层会以概率的形式表示这个图像是真实的还是合成的。在训练这个网络的时候，输入为等概率选择x^ix^i和真实数据yiyi，而最终的目的是将精炼数据标注为1，将真实数据标注为0，该式子通过随机梯度下降法来求极值。

在具体实现中，lreallreal在判别模型DD中的计算方式： l_real

具体算法如下： 具体算法

在计算精炼模型的时候计算loss function的具体实现方式为： loss_imp

总结

其实文章还有许多细节实现部分的讲解，这里就不提了，以上部分就是SimGAN的基本原理，但是你有没有发现，这其实就是基本的GAN上套了点东西，所谓的合成数据在我看来就跟CGAN和LAPGAN中的conditional variable没有差别，而且上面的具体算法部分跟Goodfellow看起来真的好像，本文章篇应用领域，不过本文在细节方面有自己的独到之处，这里没有详细述说，怎么说呢，本来以为万年闭源的apple会发出一些什么惊天动地的东西，现在看来还是有点差强人意吼，当然有人说你行你上啊，嘿嘿嘿。。。

\section{对抗神经网络在人脸属性中的应用}
从上面对抗生成网络的演变和发展来看，对抗生成网络很明显并不能像正常的CNN网络一样对于具体的模式识别任务，但是作为探究CNN生成原理的一部分，对抗生成网络主要是希望能够了解CNN能够从图像中学习到什么样的信息，怎样学习的，并且能否以较为直观的形式也就是生成图像来表示出来，（尽管学习到的东西很多时候并不能够以图像的形式进行展现）。
在本论文的实验后期，我们想到了使用对抗生成网络来探究一下CNN对于不同分布的数据集之间的学习能力。
首先引入一个经典的模式识别场景，泛化能力的问题:
在Hu的工作种，他发现一个在很多实验中都会出现的问题，使用MTL的人脸属性框架进行人脸属性识别的过程中，具同样40个属性标签的两个数据集lfwA和celeA，两个在各自数据集上训练之后的模型，在各自数据集上的准确率都很高，但是在对方的测试集效果都比较糟糕。
（加入hu的lfw和celeA的实验对比表）
如何进行改善呢?我们针对于这种情况设计了这样的思路：
问题引出：对于相同的网络模型，使用相同的训练方法，在不同数据集中的训练之后，对自身数据集的测试集准确率要远远高于其他数据集的测试集。
问题分析：首先这不是一个过拟合问题，因为对于数据集中训练集和测试集的准确率较高，所以网络的训练没有问题。但是对于不同数据集的测试集准确率很低，所以推测问题的出现是因为数据的分布不同

尝试解决办法：首先我们先假定网络模型容量可以容纳两个数据的分布（数据的分布可能不满足线性加法，但是应该满足集合性合并不减的特性，所以假定两种数据的分布集合会比原来更大，所以对于网络容量的要求会更大），既然数据的分布不同，就应当减少数据分布对于模型训练带来的影响。
	第一将两个数据集合并训练，并且采用上一章所提到的









