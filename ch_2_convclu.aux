\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{BP}
\@writefile{toc}{\contentsline {chapter}{\numberline {第\CJKnumber  {2}章}卷积神经网络的相关技术介绍}{5}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}卷积神经网络的基础操作和训练}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}卷积神经网络结构的基本组成}{5}{subsection.2.1.1}}
\citation{ALEXNET}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}卷积层}{6}{subsubsection.2.1.1.1}}
\citation{1x1CONV}
\citation{DILUTECONV}
\citation{DEFORMCONV}
\@writefile{lof}{\contentsline {figure}{\numberline {2-1}{\ignorespaces 卷积操作的示意图\relax }}{7}{figure.caption.12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.2}池化层}{7}{subsubsection.2.1.1.2}}
\citation{ROIPOOLING}
\citation{SPPNET}
\citation{VGGNET}
\citation{1x1CONV}
\citation{1x1CONV}
\citation{FCGOOD}
\@writefile{lof}{\contentsline {figure}{\numberline {2-2}{\ignorespaces max-pooling的操作示意图\relax }}{9}{figure.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.3}全连接层}{9}{subsubsection.2.1.1.3}}
\citation{BN}
\@writefile{lof}{\contentsline {figure}{\numberline {2-3}{\ignorespaces 全连接层的操作示意图\relax }}{10}{figure.caption.14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.4}归一化层}{10}{subsubsection.2.1.1.4}}
\citation{FASTERRCNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2-4}{\ignorespaces BN层的操作说明\relax }}{11}{figure.caption.15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.5}损失函数loss层}{11}{subsubsection.2.1.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}卷积神经网络常用激活函数}{12}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-5}{\ignorespaces 激活函数的具体表达式以及出现时间\relax }}{12}{figure.caption.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-6}{\ignorespaces sigmoid(a)、relu(b)、prelu(c)函数的函数曲线示意图\relax }}{12}{figure.caption.17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.1}sigmoid函数}{12}{subsubsection.2.1.2.1}}
\citation{XAVIER}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.2}relu函数}{13}{subsubsection.2.1.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.3}prelu函数}{13}{subsubsection.2.1.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}卷积神经网络常用的参数初始化方法}{13}{subsection.2.1.3}}
\citation{MSRA}
\citation{GABOR}
\@writefile{lof}{\contentsline {figure}{\numberline {2-7}{\ignorespaces 使用gabor固定初始化训练对于消耗能源的降低\relax }}{14}{figure.caption.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}卷积神经网络的训练与优化}{14}{subsection.2.1.4}}
\citation{CUDA}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}神经网络训练速度的提升}{15}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}并行模式}{15}{subsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-8}{\ignorespaces 模型并行与数据并行示意图\relax }}{16}{figure.caption.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}参数更新方式}{16}{subsection.2.2.2}}
\citation{CAFFE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}基于机器学习框架的多机多卡训练}{17}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3.1}caffe中的多机多卡}{17}{subsubsection.2.2.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}神经网络前馈速度优化}{18}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-9}{\ignorespaces 使用7层FOR循环实现卷积操作\relax }}{19}{figure.caption.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}卷积计算的优化方式}{19}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.1}通过编程优化卷积速度}{19}{subsubsection.2.3.1.1}}
\citation{CAFFE}
\citation{WINOGRAD}
\citation{MEC}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.2}卷积的快速算法}{20}{subsubsection.2.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-10}{\ignorespaces im2col的操作示意图\relax }}{21}{figure.caption.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-11}{\ignorespaces winograd算法的算法过程\relax }}{21}{figure.caption.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-12}{\ignorespaces MEC算法的算法过程\relax }}{21}{figure.caption.23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.3}借助第三方的计算库对于卷积计算进行优化}{21}{subsubsection.2.3.1.3}}
\citation{MKL}
\citation{NNPACK}
\citation{CUDNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2-13}{\ignorespaces MKL/MKL-DNN的简介\relax }}{22}{figure.caption.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-14}{\ignorespaces nnpack\relax }}{22}{figure.caption.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-15}{\ignorespaces CUDNN\relax }}{23}{figure.caption.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}不同网络层的合并}{23}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}本章小结}{23}{subsection.2.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-16}{\ignorespaces MKL、MKL2017、MKLDNN、openblas加速方法的具体速度\relax }}{24}{figure.caption.27}}
\@setckpt{ch_2_convclu}{
\setcounter{page}{25}
\setcounter{equation}{11}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{16}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{endNonectr}{41}
\setcounter{currNonectr}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{37}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{29}
\setcounter{su@anzahl}{0}
\setcounter{currassumptionctr}{0}
\setcounter{endassumptionctr}{0}
\setcounter{assumption}{0}
\setcounter{currdefinitionctr}{0}
\setcounter{enddefinitionctr}{0}
\setcounter{definition}{0}
\setcounter{currpropositionctr}{0}
\setcounter{endpropositionctr}{0}
\setcounter{proposition}{0}
\setcounter{currlemmactr}{0}
\setcounter{endlemmactr}{0}
\setcounter{lemma}{0}
\setcounter{currtheoremctr}{0}
\setcounter{endtheoremctr}{0}
\setcounter{theorem}{0}
\setcounter{curraxiomctr}{0}
\setcounter{endaxiomctr}{0}
\setcounter{axiom}{0}
\setcounter{currcorollaryctr}{0}
\setcounter{endcorollaryctr}{0}
\setcounter{corollary}{0}
\setcounter{currexamplectr}{0}
\setcounter{endexamplectr}{0}
\setcounter{example}{0}
\setcounter{currremarkctr}{0}
\setcounter{endremarkctr}{0}
\setcounter{remark}{0}
\setcounter{currproblemctr}{0}
\setcounter{endproblemctr}{0}
\setcounter{problem}{0}
\setcounter{currconjecturectr}{0}
\setcounter{endconjecturectr}{0}
\setcounter{conjecture}{0}
\setcounter{currproofctr}{0}
\setcounter{endproofctr}{0}
\setcounter{proof}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{section@level}{2}
}
